"""
Whisper Ğ´Ğ²Ğ¸Ğ¶Ğ¾Ğº Ñ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼
SOLID: Single Responsibility + Open/Closed Principle
"""

import logging
import time
import tempfile
import subprocess
import json
from typing import List, Dict, Any
from pathlib import Path

from ..interfaces.speech_recognition_interface import (
    ISpeechRecognitionEngine, 
    SpeechSegment, 
    SpeechRecognitionResult,
    ISpeechRecognitionStrategy,
    ITextSegmenter
)


class WhisperEngine(ISpeechRecognitionEngine):
    """
    OpenAI Whisper Ğ´Ğ²Ğ¸Ğ¶Ğ¾Ğº Ñ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼
    SOLID: Single Responsibility - Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Whisper Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ
    """
    
    def __init__(self, model_size: str = "tiny", logger: logging.Logger = None):
        self.model_size = model_size
        self.logger = logger or logging.getLogger(__name__)
        self._available = None
    
    def get_engine_name(self) -> str:
        return f"whisper_{self.model_size}"
    
    def is_available(self) -> bool:
        if self._available is None:
            try:
                import whisper
                import torch
                
                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
                whisper.load_model(self.model_size)
                self._available = True
            except Exception as e:
                self.logger.debug(f"Whisper Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½: {e}")
                self._available = False
        
        return self._available
    
    def get_supported_languages(self) -> List[str]:
        return ["en", "ru", "de", "fr", "es", "it", "ja", "ko", "zh", "pt", "ar"]
    
    def recognize_audio(self, audio_path: str, language: str = "en") -> SpeechRecognitionResult:
        """Ğ Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°ĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Whisper Ğ² Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ"""
        start_time = time.time()
        
        try:
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ» Ğ´Ğ»Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as result_file:
                result_path = result_file.name
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ
            script_content = f'''
import sys
import os
import json
import warnings

# ĞŸĞ¾Ğ´Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ
warnings.filterwarnings("ignore")
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["OMP_NUM_THREADS"] = "1"

print("SUBPROCESS: ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ...", flush=True)

try:
    import multiprocessing
    multiprocessing.set_start_method('spawn', force=True)
    print("SUBPROCESS: Multiprocessing ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ğ² spawn Ñ€ĞµĞ¶Ğ¸Ğ¼", flush=True)
    
    import torch
    print("SUBPROCESS: Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ torch...", flush=True)
    
    # ĞÑ‚ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ MPS Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Intel Mac
    if hasattr(torch.backends, 'mps'):
        torch.backends.mps.is_available = lambda: False
        print("SUBPROCESS: MPS Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½", flush=True)
    
    print("SUBPROCESS: Torch Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½", flush=True)
    
    import whisper
    print("SUBPROCESS: Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ {self.model_size}...", flush=True)
    
    model = whisper.load_model("{self.model_size}")
    print("SUBPROCESS: ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°", flush=True)
    
    print("SUBPROCESS: ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ±Ğ°Ñ†Ğ¸Ğ¸...", flush=True)
    result = model.transcribe(
        "{audio_path}",
        language="{language}",
        word_timestamps=True,
        verbose=False,
        temperature=0.0,
        beam_size=1,
        best_of=1,
        fp16=False
    )
    print("SUBPROCESS: Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ±Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°", flush=True)
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
    with open("{result_path}", "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print("SUBPROCESS: Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½", flush=True)
    
except Exception as e:
    print(f"SUBPROCESS ERROR: {{e}}", flush=True)
    error_result = {{
        "text": "",
        "segments": [],
        "error": str(e)
    }}
    with open("{result_path}", "w", encoding="utf-8") as f:
        json.dump(error_result, f, ensure_ascii=False, indent=2)
'''
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as script_file:
                script_file.write(script_content)
                script_path = script_file.name
            
            self.logger.info("ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº Whisper Ğ² Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ...")
            self.logger.info(f"ğŸ“„ Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½: {script_path}")
            self.logger.info(f"ğŸ“„ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑÑ Ğ²: {result_path}")
            
            # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ
            process = subprocess.run(
                ["python", script_path],
                capture_output=True,
                text=True,
                timeout=300,  # 5 Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ñ‚Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚
                env={**os.environ, "PYTHONPATH": ""}  # Ğ§Ğ¸ÑÑ‚Ğ¾Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ
            )
            
            processing_time = time.time() - start_time
            self.logger.info(f"â±ï¸ Subprocess Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ğ»ÑÑ Ğ·Ğ° {processing_time:.1f}s")
            self.logger.info(f"ğŸ” Return code: {process.returncode}")
            
            if process.stdout:
                self.logger.info(f"ğŸ“¤ Stdout: {process.stdout[:200]}...")
            
            if process.stderr:
                self.logger.error(f"ğŸ“¥ Stderr: {process.stderr[:500]}...")
            
            # Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
            if not Path(result_path).exists():
                raise RuntimeError("Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Whisper Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
            
            with open(result_path, 'r', encoding='utf-8') as f:
                whisper_result = json.load(f)
            
            # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹
            Path(script_path).unlink(missing_ok=True)
            Path(result_path).unlink(missing_ok=True)
            
            if "error" in whisper_result:
                raise RuntimeError(f"Whisper subprocess error: {whisper_result['error']}")
            
            # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Whisper Ğ² Ğ½Ğ°Ñˆ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚
            return self._convert_whisper_result(whisper_result, processing_time, language)
            
        except subprocess.TimeoutExpired:
            self.logger.error("âŒ Whisper subprocess timeout")
            raise RuntimeError("Whisper subprocess timeout")
        except Exception as e:
            self.logger.error(f"âŒ Whisper subprocess failed: {e}")
            raise
    
    def _convert_whisper_result(
        self, 
        whisper_result: Dict[str, Any], 
        processing_time: float,
        language: str
    ) -> SpeechRecognitionResult:
        """ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Whisper Ğ² Ğ½Ğ°Ñˆ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚"""
        
        segments = []
        full_text = whisper_result.get("text", "")
        
        # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Whisper
        for i, whisper_segment in enumerate(whisper_result.get("segments", [])):
            segment = SpeechSegment(
                start_time=whisper_segment.get("start", 0.0),
                end_time=whisper_segment.get("end", 0.0),
                text=whisper_segment.get("text", "").strip(),
                confidence=whisper_segment.get("avg_logprob", 0.0),  # Whisper Ğ´Ğ°ĞµÑ‚ logprob
                language=language,
                metadata={
                    "engine": "whisper",
                    "model_size": self.model_size,
                    "segment_id": i,
                    "tokens": whisper_segment.get("tokens", []),
                    "words": whisper_segment.get("words", [])
                }
            )
            segments.append(segment)
        
        # Ğ•ÑĞ»Ğ¸ Ğ½ĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¾Ğ´Ğ¸Ğ½ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹
        if not segments and full_text:
            segment = SpeechSegment(
                start_time=0.0,
                end_time=processing_time,  # ĞŸÑ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾
                text=full_text,
                confidence=0.9,
                language=language,
                metadata={
                    "engine": "whisper",
                    "model_size": self.model_size,
                    "single_segment": True
                }
            )
            segments.append(segment)
        
        total_duration = segments[-1].end_time if segments else 0.0
        
        result = SpeechRecognitionResult(
            segments=segments,
            full_text=full_text,
            total_duration=total_duration,
            engine_used=f"whisper_{self.model_size}",
            processing_time=processing_time,
            metadata={
                "model_size": self.model_size,
                "whisper_language": whisper_result.get("language"),
                "segments_count": len(segments)
            }
        )
        
        self.logger.info(f"Whisper ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾: {len(segments)} ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², '{full_text[:50]}...'")
        return result


class WhisperStrategy(ISpeechRecognitionStrategy):
    """
    Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Whisper
    SOLID: Strategy Pattern + Dependency Injection
    """
    
    def __init__(self, engine: WhisperEngine = None):
        self.engine = engine or WhisperEngine()
        self.logger = logging.getLogger(__name__)
    
    def recognize_with_segmentation(
        self, 
        audio_path: str, 
        language: str = "en",
        segmenter: ITextSegmenter = None
    ) -> SpeechRecognitionResult:
        """
        Ğ Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°ĞµÑ‚ Ñ‡ĞµÑ€ĞµĞ· Whisper
        Whisper ÑƒĞ¶Ğµ Ğ´Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ½Ğµ Ğ½ÑƒĞ¶Ğ½Ğ°
        """
        result = self.engine.recognize_audio(audio_path, language)
        
        # Whisper Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ´Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¾Ğ±ĞºĞ¸
        # ĞĞ¾ ĞµÑĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¾Ñ€ Ğ¸ ĞµÑÑ‚ÑŒ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹, Ğ¼Ğ¾Ğ¶ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ
        if segmenter and result.segments:
            long_segments = [s for s in result.segments if len(s.text) > 400]
            
            if long_segments:
                self.logger.debug(f"Whisper: Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(long_segments)} Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²")
                
                new_segments = []
                for segment in result.segments:
                    if len(segment.text) > 400:
                        # ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñƒ
                        sub_segments = segmenter.segment_text(segment.text)
                        
                        if len(sub_segments) > 1:
                            duration = segment.end_time - segment.start_time
                            sub_duration = duration / len(sub_segments)
                            
                            for i, sub_text in enumerate(sub_segments):
                                sub_start = segment.start_time + i * sub_duration
                                sub_end = segment.start_time + (i + 1) * sub_duration
                                
                                sub_segment = SpeechSegment(
                                    start_time=sub_start,
                                    end_time=sub_end,
                                    text=sub_text,
                                    confidence=segment.confidence,
                                    language=segment.language,
                                    metadata={
                                        **segment.metadata,
                                        "whisper_subsegmented": True,
                                        "original_segment_id": segment.metadata.get("segment_id")
                                    }
                                )
                                new_segments.append(sub_segment)
                        else:
                            new_segments.append(segment)
                    else:
                        new_segments.append(segment)
                
                if len(new_segments) != len(result.segments):
                    result.segments = new_segments
                    result.metadata["whisper_segmentation_applied"] = True
                    self.logger.info(f"Whisper ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ: {len(new_segments)} Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²")
        
        return result