"""
Ð“Ð»Ð°Ð²Ð½Ñ‹Ð¹ Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ‚Ð¾Ñ€ Ð²Ð¸Ð´ÐµÐ¾ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‡Ð¸ÐºÐ° Ð¿Ð¾ Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿Ð°Ð¼ SOLID
SOLID: Dependency Inversion + Single Responsibility + Open/Closed
"""

import logging
import time
from typing import Optional, Dict, Any, List
from pathlib import Path
from dataclasses import dataclass

from interfaces.speech_recognition_interface import (
    ISpeechRecognitionStrategy, 
    ITextSegmenter,
    SpeechRecognitionResult
)
from interfaces.video_output_interface import (
    IVideoOutputStrategy,
    VideoOutputFormat,
    VideoOutputConfig,
    ProcessedVideo
)
from speech_engines.factory import SpeechEngineFactory, SpeechEngineSelector
from video_outputs.factory import VideoOutputFactory
from core.audio_processor import AudioProcessor
from core.speech_synthesizer import SpeechSynthesizer
from translator_compat import translate_text


@dataclass
class TranslationConfig:
    """ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð° Ð²Ð¸Ð´ÐµÐ¾"""
    # Speech Recognition
    preferred_sr_engine: Optional[str] = None
    source_language: str = "en"
    target_language: str = "ru"
    
    # Video Output
    output_format: VideoOutputFormat = VideoOutputFormat.TRANSLATION_ONLY
    preserve_original_audio: bool = False
    
    # Quality Settings
    prioritize_quality: bool = True
    prioritize_speed: bool = False
    offline_mode: bool = False
    
    # Advanced
    custom_segmentation: bool = True
    max_segment_length: int = 400


@dataclass
class TranslationResult:
    """Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð° Ð²Ð¸Ð´ÐµÐ¾"""
    success: bool
    output_video: Optional[ProcessedVideo]
    speech_result: Optional[SpeechRecognitionResult]
    error_message: Optional[str]
    processing_time: float
    stats: Dict[str, Any]


class VideoTranslatorSOLID:
    """
    Ð“Ð»Ð°Ð²Ð½Ñ‹Ð¹ ÐºÐ»Ð°ÑÑ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‡Ð¸ÐºÐ° Ð²Ð¸Ð´ÐµÐ¾ Ð¿Ð¾ Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿Ð°Ð¼ SOLID
    
    SOLID Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿Ñ‹:
    - Single Responsibility: Ð¢Ð¾Ð»ÑŒÐºÐ¾ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ° Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð°
    - Open/Closed: Ð Ð°ÑÑˆÐ¸Ñ€ÑÐµÑ‚ÑÑ Ð½Ð¾Ð²Ñ‹Ð¼Ð¸ Ð´Ð²Ð¸Ð¶ÐºÐ°Ð¼Ð¸ Ñ‡ÐµÑ€ÐµÐ· Ñ„Ð°Ð±Ñ€Ð¸ÐºÐ¸
    - Liskov Substitution: Ð’ÑÐµ Ð´Ð²Ð¸Ð¶ÐºÐ¸ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð·Ð°Ð¼ÐµÐ½ÑÐµÐ¼Ñ‹ Ñ‡ÐµÑ€ÐµÐ· Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹ÑÑ‹  
    - Interface Segregation: Ð§ÐµÑ‚ÐºÐ¸Ðµ Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹ÑÑ‹ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð·Ð°Ð´Ð°Ñ‡Ð¸
    - Dependency Inversion: Ð—Ð°Ð²Ð¸ÑÐ¸Ñ‚ Ð¾Ñ‚ Ð°Ð±ÑÑ‚Ñ€Ð°ÐºÑ†Ð¸Ð¹, Ð½Ðµ Ð¾Ñ‚ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ñ… ÐºÐ»Ð°ÑÑÐ¾Ð²
    """
    
    def __init__(self,
                 speech_factory: Optional[SpeechEngineFactory] = None,
                 video_factory: Optional[VideoOutputFactory] = None,
                 audio_processor: Optional[AudioProcessor] = None,
                 speech_synthesizer: Optional[SpeechSynthesizer] = None,
                 logger: Optional[logging.Logger] = None):
        """
        Dependency Injection - Ð²ÑÐµ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¿ÐµÑ€ÐµÐ´Ð°ÑŽÑ‚ÑÑ Ð¸Ð·Ð²Ð½Ðµ
        """
        self.speech_factory = speech_factory or SpeechEngineFactory()
        self.video_factory = video_factory or VideoOutputFactory()
        self.engine_selector = SpeechEngineSelector(self.speech_factory)
        self.audio_processor = audio_processor or AudioProcessor()
        self.speech_synthesizer = speech_synthesizer or SpeechSynthesizer()
        self.logger = logger or logging.getLogger(__name__)
        
        self.logger.info("VideoTranslatorSOLID Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ñ Ð¼Ð¾Ð´ÑƒÐ»ÑŒÐ½Ð¾Ð¹ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð¾Ð¹")
    
    def get_available_engines(self) -> List[str]:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ Ð´Ð²Ð¸Ð¶ÐºÐ¸ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ"""
        return self.speech_factory.get_available_engines()
    
    def get_available_output_formats(self) -> List[VideoOutputFormat]:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ñ‹ Ð²Ñ‹Ð²Ð¾Ð´Ð°"""
        return self.video_factory.get_available_formats()
    
    def get_format_descriptions(self) -> Dict[VideoOutputFormat, str]:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¾Ð² Ð´Ð»Ñ UI"""
        formats = self.get_available_output_formats()
        return {fmt: self.video_factory.get_format_description(fmt) for fmt in formats}
    
    def translate_video(self, 
                       input_video_path: str,
                       output_video_path: str, 
                       config: TranslationConfig) -> TranslationResult:
        """
        Ð“Ð»Ð°Ð²Ð½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð° Ð²Ð¸Ð´ÐµÐ¾
        
        SOLID: Single Responsibility - Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð¸Ñ€ÑƒÐµÑ‚ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ
        """
        start_time = time.time()
        
        try:
            self.logger.info(f"ðŸŽ¬ ÐÐ°Ñ‡Ð°Ð»Ð¾ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð° Ð²Ð¸Ð´ÐµÐ¾: {input_video_path}")
            self.logger.info(f"ðŸ“‹ ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ: SR={config.preferred_sr_engine}, "
                           f"Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚={config.output_format}, ÐšÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾={config.prioritize_quality}")
            
            # 1. Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð°ÑƒÐ´Ð¸Ð¾
            audio_path = self._extract_audio(input_video_path)
            
            # 2. Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð´Ð²Ð¸Ð¶Ð¾Ðº Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ
            selected_engine = self._select_speech_engine(config, audio_path)
            
            # 3. Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°ÐµÐ¼ Ñ€ÐµÑ‡ÑŒ Ñ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸ÐµÐ¹
            speech_result = self._recognize_speech(audio_path, selected_engine, config)
            
            # 4. ÐŸÐµÑ€ÐµÐ²Ð¾Ð´Ð¸Ð¼ Ð¸ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€ÑƒÐµÐ¼ Ñ€ÐµÑ‡ÑŒ
            translated_segments = self._translate_and_synthesize(speech_result, config)
            
            # 5. Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð²Ð¸Ð´ÐµÐ¾ Ð² Ð½ÑƒÐ¶Ð½Ð¾Ð¼ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ
            output_video = self._create_output_video(
                input_video_path, translated_segments, output_video_path, config
            )
            
            processing_time = time.time() - start_time
            
            result = TranslationResult(
                success=True,
                output_video=output_video,
                speech_result=speech_result,
                error_message=None,
                processing_time=processing_time,
                stats={
                    "engine_used": selected_engine,
                    "segments_processed": len(translated_segments),
                    "output_format": config.output_format,
                    "has_subtitles": output_video.has_subtitles,
                    "file_size_mb": output_video.file_size_mb
                }
            )
            
            self.logger.info(f"âœ… ÐŸÐµÑ€ÐµÐ²Ð¾Ð´ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð° {processing_time:.1f}s")
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            self.logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð°: {error_msg}")
            
            return TranslationResult(
                success=False,
                output_video=None,
                speech_result=None,
                error_message=error_msg,
                processing_time=processing_time,
                stats={"error": True}
            )
    
    def _extract_audio(self, video_path: str) -> str:
        """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ð°ÑƒÐ´Ð¸Ð¾ Ð¸Ð· Ð²Ð¸Ð´ÐµÐ¾"""
        self.logger.info("ðŸŽµ Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð°ÑƒÐ´Ð¸Ð¾...")
        return self.audio_processor.extract_audio_from_video(video_path)
    
    def _select_speech_engine(self, config: TranslationConfig, audio_path: str) -> str:
        """
        Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÑ‚ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð´Ð²Ð¸Ð¶Ð¾Ðº Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ
        
        SOLID: Dependency Inversion - Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð°Ð±ÑÑ‚Ñ€Ð°ÐºÑ†Ð¸ÑŽ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ð°
        """
        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð°ÑƒÐ´Ð¸Ð¾ Ð´Ð»Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        from pydub import AudioSegment
        audio = AudioSegment.from_file(audio_path)
        duration = len(audio) / 1000.0
        
        selected = self.engine_selector.select_best_engine(
            user_preference=config.preferred_sr_engine,
            audio_duration=duration,
            quality_needed=config.prioritize_quality
        )
        
        self.logger.info(f"ðŸŽ¯ Ð’Ñ‹Ð±Ñ€Ð°Ð½ Ð´Ð²Ð¸Ð¶Ð¾Ðº: {selected} (Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ: {duration:.1f}s)")
        return selected
    
    def _recognize_speech(self, 
                         audio_path: str, 
                         engine_name: str,
                         config: TranslationConfig) -> SpeechRecognitionResult:
        """
        Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°ÐµÑ‚ Ñ€ÐµÑ‡ÑŒ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸ÐµÐ¹
        
        SOLID: Open/Closed - Ð½Ð¾Ð²Ñ‹Ðµ Ð´Ð²Ð¸Ð¶ÐºÐ¸ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÑŽÑ‚ÑÑ Ð±ÐµÐ· Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ ÐºÐ¾Ð´Ð°
        """
        self.logger.info(f"ðŸŽ¤ Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÑ‡Ð¸ Ñ‡ÐµÑ€ÐµÐ· {engine_name}...")
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑŽ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ
        strategy = self.speech_factory.create_strategy(engine_name)
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ‚Ð¾Ñ€ ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð° ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ
        segmenter = None
        if config.custom_segmentation:
            segmenter = self.speech_factory.create_segmenter("auto")
        
        # Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°ÐµÐ¼ Ñ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸ÐµÐ¹
        result = strategy.recognize_with_segmentation(
            audio_path, 
            config.source_language,
            segmenter
        )
        
        self.logger.info(f"âœ… Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð½Ð¾: {len(result.segments)} ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð², "
                        f"'{result.full_text[:50]}...' Ð·Ð° {result.processing_time:.1f}s")
        
        return result
    
    def _translate_and_synthesize(self, 
                                speech_result: SpeechRecognitionResult,
                                config: TranslationConfig) -> List[Dict[str, Any]]:
        """ÐŸÐµÑ€ÐµÐ²Ð¾Ð´Ð¸Ñ‚ Ð¸ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€ÑƒÐµÑ‚ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚"""
        self.logger.info(f"ðŸŒ ÐŸÐµÑ€ÐµÐ²Ð¾Ð´ Ð¸ ÑÐ¸Ð½Ñ‚ÐµÐ· {len(speech_result.segments)} ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð²...")
        
        translated_segments = []
        
        for i, segment in enumerate(speech_result.segments):
            if not segment.text.strip():
                continue
            
            self.logger.debug(f"Ð¡ÐµÐ³Ð¼ÐµÐ½Ñ‚ {i+1}: '{segment.text[:30]}...'")
            
            # ÐŸÐµÑ€ÐµÐ²Ð¾Ð´Ð¸Ð¼ Ñ‚ÐµÐºÑÑ‚
            translated_text = translate_text(
                segment.text, 
                config.source_language, 
                config.target_language
            )
            
            # Ð¡Ð¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€ÑƒÐµÐ¼ Ñ€ÐµÑ‡ÑŒ
            audio_path = None
            if config.output_format in [VideoOutputFormat.TRANSLATION_ONLY, 
                                      VideoOutputFormat.TRANSLATION_WITH_SUBTITLES]:
                audio_path = self.speech_synthesizer.synthesize_speech(
                    translated_text, 
                    config.target_language
                )
            
            segment_data = {
                "start_time": segment.start_time,
                "end_time": segment.end_time,
                "original_text": segment.text,
                "translated_text": translated_text,
                "translated_audio_path": audio_path,
                "confidence": segment.confidence,
                "language": segment.language,
                "metadata": segment.metadata
            }
            
            translated_segments.append(segment_data)
        
        self.logger.info(f"âœ… ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ {len(translated_segments)} ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð²")
        return translated_segments
    
    def _create_output_video(self,
                            input_video_path: str,
                            translated_segments: List[Dict[str, Any]], 
                            output_path: str,
                            config: TranslationConfig) -> ProcessedVideo:
        """
        Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð²Ð¸Ð´ÐµÐ¾ Ð² Ð½ÑƒÐ¶Ð½Ð¾Ð¼ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ
        
        SOLID: Strategy Pattern - Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑŽ
        """
        self.logger.info(f"ðŸŽ¬ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð²Ð¸Ð´ÐµÐ¾ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ: {config.output_format}")
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑŽ Ð´Ð»Ñ Ð½ÑƒÐ¶Ð½Ð¾Ð³Ð¾ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð°
        strategy = self.video_factory.create_strategy(config.output_format)
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ Ð²Ñ‹Ð²Ð¾Ð´Ð°
        output_config = VideoOutputConfig(
            output_format=config.output_format,
            subtitle_language=config.target_language,
            audio_language=config.target_language,
            preserve_original_audio=config.preserve_original_audio
        )
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð²Ð¸Ð´ÐµÐ¾
        result = strategy.create_output(
            input_video_path,
            translated_segments, 
            output_path,
            output_config
        )
        
        return result
    
    def get_system_status(self) -> Dict[str, Any]:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÑÑ‚Ð°Ñ‚ÑƒÑ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð´Ð»Ñ Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ¸"""
        available_engines = self.get_available_engines()
        available_formats = self.get_available_output_formats()
        
        return {
            "speech_engines": {
                "available": available_engines,
                "total": len(available_engines),
                "recommended": self.engine_selector.factory.get_recommended_engine(quality_priority=True)
            },
            "output_formats": {
                "available": [fmt.value for fmt in available_formats],
                "descriptions": {fmt.value: self.video_factory.get_format_description(fmt) 
                              for fmt in available_formats}
            },
            "components": {
                "audio_processor": self.audio_processor is not None,
                "speech_synthesizer": self.speech_synthesizer is not None,
                "speech_factory": len(available_engines) > 0,
                "video_factory": len(available_formats) > 0
            }
        }