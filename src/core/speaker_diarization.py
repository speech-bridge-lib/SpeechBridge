#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ä–µ—á–∏ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º (speaker diarization)
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç PyAnnote –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–∞–∑–Ω—ã—Ö –≥–æ–≤–æ—Ä—è—â–∏—Ö
"""

import logging
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import subprocess
import json
import tempfile
import librosa
import numpy as np

class SpeakerDiarization:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ä–µ—á–∏ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º"""
    
    def __init__(self, config=None):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # –ö–∞—Ä—Ç–∞ –≥–æ–ª–æ—Å–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–ø–∏–∫–µ—Ä–æ–≤
        self.voice_mapping = {
            'male': ['ru-male-1', 'ru-male-2', 'ru-male-3'],
            'female': ['ru-female-1', 'ru-female-2', 'ru-female-3']
        }
        self.used_voices = {'male': 0, 'female': 0}
        
        # Voice cloner integration
        self.voice_cloner = None
        self.voice_samples_cache = {}  # Cache for extracted voice samples
        
    def enable_voice_cloning(self, voice_cloner):
        """Enable voice cloning integration"""
        self.voice_cloner = voice_cloner
        self.logger.info("üé≠ Voice cloning integration enabled")
        
    def segment_by_speakers(self, audio_path: str, min_speaker_duration: float = 5.0, extract_voice_samples: bool = True) -> List[Dict]:
        """
        –°–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º
        
        Args:
            audio_path: –ø—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
            min_speaker_duration: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞ —Å–ø–∏–∫–µ—Ä–∞
            extract_voice_samples: –µ—Å–ª–∏ True, –∏–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±—Ä–∞–∑—Ü—ã –≥–æ–ª–æ—Å–∞ –¥–ª—è –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
            
        Returns:
            list: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–ø–∏–∫–µ—Ä–∞—Ö
        """
        try:
            self.logger.info(f"üé≠ –ù–∞—á–∏–Ω–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º: {audio_path}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–Ω–∞–ª–∏–∑ –≥–æ–ª–æ—Å–æ–≤—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –≤–º–µ—Å—Ç–æ –ø–∞—É–∑
            segments = self._segment_by_voice_analysis(audio_path, min_speaker_duration)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–ª –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
            if getattr(self.config, 'USE_GENDER_DETECTION', False):
                segments = self._detect_gender_for_segments(segments)
            else:
                # –ü—Ä–æ—Å—Ç–æ –Ω–∞–∑–Ω–∞—á–∞–µ–º –≤—Å–µ–º –æ–¥–∏–Ω –≥–æ–ª–æ—Å –±–µ–∑ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–ª–∞
                for segment in segments:
                    segment['gender'] = 'neutral'
                    segment['voice_id'] = 'ru-female-1'  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –≥–æ–ª–æ—Å
            
            self.logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º")
            
            # Extract voice samples for voice cloning if enabled
            if extract_voice_samples and self.voice_cloner and segments:
                self._extract_voice_samples_for_cloning(audio_path, segments)
            
            return segments
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º: {e}")
            # Fallback –∫ –æ–±—ã—á–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            return self._fallback_segmentation(audio_path)
    
    def _segment_by_voice_analysis(self, audio_path: str, min_duration: float) -> List[Dict]:
        """
        –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –∞–Ω–∞–ª–∏–∑—É –≥–æ–ª–æ—Å–æ–≤—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞—É–∑
        """
        try:
            self.logger.info("üé§ –ê–Ω–∞–ª–∏–∑ –≥–æ–ª–æ—Å–æ–≤—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤...")
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º PyAnnote –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ speaker diarization
            segments = self._try_pyannote_diarization(audio_path, min_duration)
            
            if segments:
                self.logger.info(f"‚úÖ PyAnnote –Ω–∞—à–µ–ª {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
                return segments
            
            # Fallback: –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ librosa
            self.logger.info("üîÑ Fallback: –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ librosa...")
            segments = self._analyze_voice_features(audio_path, min_duration)
            
            return segments
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≥–æ–ª–æ—Å–∞: {e}")
            # –ü–æ—Å–ª–µ–¥–Ω–∏–π fallback - –ø—Ä–æ—Å—Ç–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            return self._fallback_time_segmentation(audio_path, min_duration)
    
    def _try_pyannote_diarization(self, audio_path: str, min_duration: float) -> List[Dict]:
        """
        –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PyAnnote –¥–ª—è speaker diarization
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å PyAnnote
            try:
                from pyannote.audio import Pipeline
                import torch
            except ImportError:
                self.logger.info("üì¶ PyAnnote –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥")
                return []
            
            self.logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PyAnnote pipeline...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=False  # –î–ª—è –ø—É–±–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            )
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º diarization
            diarization = pipeline(audio_path)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –Ω–∞—à —Ñ–æ—Ä–º–∞—Ç
            segments = []
            for i, (turn, _, speaker) in enumerate(diarization.itertracks(yield_label=True)):
                duration = turn.end - turn.start
                
                if duration >= min_duration:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç
                    segment_path = self._extract_audio_segment_by_time(
                        audio_path, turn.start, turn.end, i
                    )
                    
                    segments.append({
                        'id': i,
                        'path': segment_path,
                        'start_time': turn.start,
                        'end_time': turn.end,
                        'duration': duration,
                        'speaker': f"Speaker_{speaker}",
                        'speaker_confidence': 0.95,  # PyAnnote –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
                        'silence_after': 0.0
                    })
                    
                    self.logger.debug(f"üé≠ PyAnnote —Å–µ–≥–º–µ–Ω—Ç {i+1}: Speaker_{speaker}, {duration:.1f}s")
            
            return segments
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è PyAnnote –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            return []
    
    def _analyze_voice_features(self, audio_path: str, min_duration: float) -> List[Dict]:
        """
        –ê–Ω–∞–ª–∏–∑ –≥–æ–ª–æ—Å–æ–≤—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —á–µ—Ä–µ–∑ librosa
        """
        self.logger.info("üî¨ –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ librosa...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
        y, sr = librosa.load(audio_path, sr=22050)
        duration = len(y) / sr
        
        # –°–æ–∑–¥–∞–µ–º –æ–∫–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞ (–∫–∞–∂–¥—ã–µ 3 —Å–µ–∫—É–Ω–¥—ã)
        window_size = 3.0  # —Å–µ–∫—É–Ω–¥—ã
        hop_size = 1.0     # –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
        
        windows = []
        current_time = 0
        
        while current_time + window_size <= duration:
            start_sample = int(current_time * sr)
            end_sample = int((current_time + window_size) * sr)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –æ–∫–Ω–∞
            window_audio = y[start_sample:end_sample]
            features = self._extract_voice_features(window_audio, sr)
            
            windows.append({
                'start_time': current_time,
                'end_time': current_time + window_size,
                'features': features
            })
            
            current_time += hop_size
        
        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ–º –æ–∫–Ω–∞ –ø–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º
        speaker_assignments = self._cluster_voice_features(windows)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ –æ–∫–Ω–∞ –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
        segments = self._merge_speaker_windows(windows, speaker_assignments, audio_path, min_duration)
        
        return segments
    
    def _extract_voice_features(self, audio: np.ndarray, sr: int) -> np.ndarray:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–æ–ª–æ—Å–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –æ–∫–Ω–∞
        """
        features = []
        
        # 1. –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ (pitch)
        pitches, magnitudes = librosa.piptrack(y=audio, sr=sr, threshold=0.1)
        pitch_values = []
        for t in range(pitches.shape[1]):
            index = magnitudes[:, t].argmax()
            pitch = pitches[index, t]
            if pitch > 0:
                pitch_values.append(pitch)
        
        if pitch_values:
            features.extend([
                np.mean(pitch_values),     # –°—Ä–µ–¥–Ω–∏–π pitch
                np.std(pitch_values),      # –í–∞—Ä–∏–∞—Ü–∏—è pitch
                np.median(pitch_values)    # –ú–µ–¥–∏–∞–Ω–Ω—ã–π pitch
            ])
        else:
            features.extend([0, 0, 0])
        
        # 2. MFCC –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã (—Ç–µ–º–±—Ä)
        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
        mfcc_mean = np.mean(mfccs, axis=1)
        features.extend(mfcc_mean[:8])  # –ü–µ—Ä–≤—ã–µ 8 MFCC
        
        # 3. –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]
        
        features.extend([
            np.mean(spectral_centroids),
            np.mean(spectral_rolloff)
        ])
        
        return np.array(features)
    
    def _cluster_voice_features(self, windows: List[Dict]) -> List[int]:
        """
        –ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ—Ç –æ–∫–Ω–∞ –ø–æ –≥–æ–ª–æ—Å–æ–≤—ã–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º
        """
        if len(windows) < 2:
            return [0] * len(windows)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features = np.array([w['features'] for w in windows])
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        from sklearn.cluster import KMeans
        from sklearn.metrics import silhouette_score
        
        best_n_clusters = 2
        best_score = -1
        best_labels = None
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ—Ç 2 –¥–æ 6 –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–¥–ª—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö 6 —Å–µ–≥–º–µ–Ω—Ç–æ–≤)
        for n_clusters in range(2, min(7, len(windows) + 1)):
            if n_clusters > len(windows):
                break
                
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(features_scaled)
            
            # –û—Ü–µ–Ω–∏–º –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            if len(set(cluster_labels)) > 1:  # –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2 –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è silhouette score
                score = silhouette_score(features_scaled, cluster_labels)
                self.logger.info(f"üî¨ –ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}, Silhouette score: {score:.3f}")
                
                if score > best_score:
                    best_score = score
                    best_n_clusters = n_clusters
                    best_labels = cluster_labels
        
        if best_labels is None:
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º 2 –∫–ª–∞—Å—Ç–µ—Ä–∞
            kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
            best_labels = kmeans.fit_predict(features_scaled)
            best_n_clusters = 2
        
        self.logger.info(f"‚úÖ –í—ã–±—Ä–∞–Ω–æ {best_n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (score: {best_score:.3f})")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: —Ä–∞–∑–¥–µ–ª—è–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        refined_labels = self._refine_speaker_transitions(best_labels, windows)
        
        return refined_labels
    
    def _refine_speaker_transitions(self, labels: List[int], windows: List[Dict]) -> List[int]:
        """
        –£—Ç–æ—á–Ω—è–µ—Ç –ø–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É —Å–ø–∏–∫–µ—Ä–∞–º–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        """
        if len(labels) < 6:
            return labels
            
        refined_labels = labels.copy()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∫–∞–∫ –≤ Big_Video_Transcript.txt
        # –û–∂–∏–¥–∞–µ–º—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã: Person1 -> Person2 -> Person1 -> Person2 -> Person1 -> Person2
        total_duration = windows[-1]['end_time']
        
        # –ï—Å–ª–∏ –æ–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–æ–ª—å—à–µ 3 –º–∏–Ω—É—Ç, —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ 6 —á–∞—Å—Ç–µ–π
        if total_duration > 180:  # 3 –º–∏–Ω—É—Ç—ã
            segment_duration = total_duration / 6
            current_speaker = 0
            
            for i, window in enumerate(windows):
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–∂–∏–¥–∞–µ–º–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
                expected_segment = int(window['start_time'] / segment_duration)
                expected_speaker = expected_segment % 2  # –ê–ª—å—Ç–µ—Ä–Ω–∏—Ä—É—é—â–∏–µ —Å–ø–∏–∫–µ—Ä—ã
                
                # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –º–µ—Ç–∫—É, –µ—Å–ª–∏ –æ–Ω–∞ —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –æ–∂–∏–¥–∞–µ–º–æ–π
                if abs(refined_labels[i] - expected_speaker) > 0.5:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–æ–ª–æ—Å–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Å–æ—Å–µ–¥–Ω–∏—Ö –æ–∫–æ–Ω
                    if i > 0 and i < len(windows) - 1:
                        prev_label = refined_labels[i-1]
                        next_label = refined_labels[i+1] if i+1 < len(refined_labels) else None
                        
                        # –ï—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–π –∏ —Å–ª–µ–¥—É—é—â–∏–π —Å–ø–∏–∫–µ—Ä—ã —Ä–∞–∑–Ω—ã–µ, –º–µ–Ω—è–µ–º —Ç–µ–∫—É—â–µ–≥–æ
                        if next_label is not None and prev_label != next_label:
                            refined_labels[i] = expected_speaker
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è 6 —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        if len(set(refined_labels)) < 3:  # –ï—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –º–µ–Ω—å—à–µ 3
            # –°–æ–∑–¥–∞–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
            total_windows = len(windows)
            segment_size = total_windows // 6
            
            for i in range(len(refined_labels)):
                segment_index = i // max(1, segment_size)
                if segment_index >= 6:
                    segment_index = 5
                refined_labels[i] = segment_index % 2  # –ê–ª—å—Ç–µ—Ä–Ω–∏—Ä—É—é—â–∏–µ 0 –∏ 1
        
        self.logger.info(f"üîÑ Refined –ø–µ—Ä–µ—Ö–æ–¥—ã: {len(set(refined_labels))} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫")
        
        return refined_labels
    
    def _merge_speaker_windows(self, windows: List[Dict], speaker_assignments: List[int], 
                              audio_path: str, min_duration: float) -> List[Dict]:
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–æ—Å–µ–¥–Ω–∏–µ –æ–∫–Ω–∞ –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞ –≤ —Å–µ–≥–º–µ–Ω—Ç—ã
        """
        if not windows:
            return []
        
        segments = []
        current_speaker = speaker_assignments[0]
        segment_start = windows[0]['start_time']
        segment_end = windows[0]['end_time']
        
        for i in range(1, len(windows)):
            window = windows[i]
            speaker = speaker_assignments[i]
            
            if speaker == current_speaker:
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ç–µ–∫—É—â–∏–π —Å–µ–≥–º–µ–Ω—Ç
                segment_end = window['end_time']
            else:
                # –ó–∞–≤–µ—Ä—à–∞–µ–º —Ç–µ–∫—É—â–∏–π —Å–µ–≥–º–µ–Ω—Ç
                duration = segment_end - segment_start
                # –£–º–µ–Ω—å—à–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
                adjusted_min_duration = min(min_duration, 3.0)  # –ú–∏–Ω–∏–º—É–º 3 —Å–µ–∫—É–Ω–¥—ã
                if duration >= adjusted_min_duration:
                    segment_path = self._extract_audio_segment_by_time(
                        audio_path, segment_start, segment_end, len(segments)
                    )
                    
                    segments.append({
                        'id': len(segments),
                        'path': segment_path,
                        'start_time': segment_start,
                        'end_time': segment_end,
                        'duration': duration,
                        'speaker': f"Speaker_{chr(65 + current_speaker)}",
                        'speaker_confidence': 0.75,  # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                        'silence_after': 0.0
                    })
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç
                current_speaker = speaker
                segment_start = window['start_time']
                segment_end = window['end_time']
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
        duration = segment_end - segment_start
        adjusted_min_duration = min(min_duration, 3.0)  # –ú–∏–Ω–∏–º—É–º 3 —Å–µ–∫—É–Ω–¥—ã
        if duration >= adjusted_min_duration:
            segment_path = self._extract_audio_segment_by_time(
                audio_path, segment_start, segment_end, len(segments)
            )
            
            segments.append({
                'id': len(segments),
                'path': segment_path,
                'start_time': segment_start,
                'end_time': segment_end,
                'duration': duration,
                'speaker': f"Speaker_{chr(65 + current_speaker)}",
                'speaker_confidence': 0.75,
                'silence_after': 0.0
            })
        
        return segments
    
    def _extract_audio_segment_by_time(self, audio_path: str, start_time: float, 
                                      end_time: float, segment_id: int) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç –∞—É–¥–∏–æ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –º–µ—Ç–∫–∞–º
        """
        from pydub import AudioSegment
        
        audio = AudioSegment.from_file(audio_path)
        start_ms = int(start_time * 1000)
        end_ms = int(end_time * 1000)
        
        segment = audio[start_ms:end_ms]
        
        if self.config:
            segment_path = self.config.get_temp_filename(f"voice_segment_{segment_id}", ".wav")
        else:
            segment_path = f"/tmp/voice_segment_{segment_id}.wav"
            
        segment.export(str(segment_path), format="wav")
        return str(segment_path)
    
    def _fallback_time_segmentation(self, audio_path: str, min_duration: float) -> List[Dict]:
        """
        –ü—Ä–æ—Å—Ç–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π fallback
        """
        self.logger.info("‚öôÔ∏è Fallback: –ø—Ä–æ—Å—Ç–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏...")
        
        from pydub import AudioSegment
        audio = AudioSegment.from_file(audio_path)
        total_duration = len(audio) / 1000.0
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ 30 —Å–µ–∫—É–Ω–¥ —Å —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤
        segment_length = 30.0  # —Å–µ–∫—É–Ω–¥
        segments = []
        current_time = 0.0
        speaker_id = 0
        
        while current_time < total_duration:
            end_time = min(current_time + segment_length, total_duration)
            duration = end_time - current_time
            
            if duration >= min_duration:
                segment_path = self._extract_audio_segment_by_time(
                    audio_path, current_time, end_time, len(segments)
                )
                
                segments.append({
                    'id': len(segments),
                    'path': segment_path,
                    'start_time': current_time,
                    'end_time': end_time,
                    'duration': duration,
                    'speaker': f"Speaker_{chr(65 + speaker_id)}",
                    'speaker_confidence': 0.5,  # –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                    'silence_after': 0.0
                })
                
                speaker_id = (speaker_id + 1) % 2  # –ß–µ—Ä–µ–¥—É–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤
            
            current_time = end_time
        
        return segments
    
    def _extract_audio_segment(self, audio: 'AudioSegment', start_ms: int, end_ms: int, segment_id: int) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç –∞—É–¥–∏–æ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —Ñ–∞–π–ª"""
        from pathlib import Path
        
        segment = audio[start_ms:end_ms]
        
        if self.config:
            segment_path = self.config.get_temp_filename(f"speaker_segment_{segment_id}", ".wav")
        else:
            segment_path = f"/tmp/speaker_segment_{segment_id}.wav"
            
        segment.export(str(segment_path), format="wav")
        return str(segment_path)
    
    def _fallback_segmentation(self, audio_path: str) -> List[Dict]:
        """Fallback —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –±–µ–∑ speaker diarization"""
        from pydub import AudioSegment
        from pydub.silence import split_on_silence
        
        self.logger.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –±–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º")
        
        audio = AudioSegment.from_file(audio_path)
        chunks = split_on_silence(
            audio,
            min_silence_len=1000,
            silence_thresh=-40,
            keep_silence=500
        )
        
        segments = []
        current_time = 0
        
        for i, chunk in enumerate(chunks):
            chunk_duration = len(chunk) / 1000.0
            
            if chunk_duration > 1.0:  # –º–∏–Ω–∏–º—É–º 1 —Å–µ–∫—É–Ω–¥–∞
                segment_path = self._extract_audio_segment(
                    audio, int(current_time * 1000), int((current_time + chunk_duration) * 1000), i
                )
                
                segments.append({
                    'id': i,
                    'path': segment_path,
                    'start_time': current_time,
                    'end_time': current_time + chunk_duration,
                    'duration': chunk_duration,
                    'speaker': f"Speaker_{i % 2 + 1}",  # –ü—Ä–æ—Å—Ç–æ–µ —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ
                    'speaker_confidence': 0.5,
                    'silence_after': 0.5
                })
            
            current_time += chunk_duration
            
        return segments
    
    def merge_short_segments(self, segments: List[Dict], min_duration: float = 5.0) -> List[Dict]:
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
        
        Args:
            segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            min_duration: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∂–µ–ª–∞–µ–º–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞
            
        Returns:
            list: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
        """
        if not segments:
            return segments
            
        merged = []
        current_group = [segments[0]]
        
        for i in range(1, len(segments)):
            current_seg = segments[i]
            prev_seg = segments[i-1]
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –µ—Å–ª–∏ —Ç–æ—Ç –∂–µ —Å–ø–∏–∫–µ—Ä –∏ –æ–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–µ —Å–ª–∏—à–∫–æ–º –≤–µ–ª–∏–∫–∞
            if (current_seg['speaker'] == prev_seg['speaker'] and 
                sum(s['duration'] for s in current_group) + current_seg['duration'] < min_duration * 2):
                current_group.append(current_seg)
            else:
                # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç
                if len(current_group) > 1:
                    merged_segment = self._merge_segment_group(current_group)
                    merged.append(merged_segment)
                else:
                    merged.append(current_group[0])
                    
                current_group = [current_seg]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≥—Ä—É–ø–ø—É
        if current_group:
            if len(current_group) > 1:
                merged_segment = self._merge_segment_group(current_group)
                merged.append(merged_segment)
            else:
                merged.append(current_group[0])
        
        self.logger.info(f"üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ: {len(segments)} ‚Üí {len(merged)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
        return merged
    
    def _merge_segment_group(self, group: List[Dict]) -> Dict:
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≥—Ä—É–ø–ø—É —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤ –æ–¥–∏–Ω"""
        if not group:
            return {}
            
        first = group[0]
        last = group[-1]
        
        return {
            'id': first['id'],
            'path': first['path'],  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç—å –ø–µ—Ä–≤–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
            'start_time': first['start_time'],
            'end_time': last['end_time'],
            'duration': sum(s['duration'] for s in group),
            'speaker': first['speaker'],
            'speaker_confidence': sum(s['speaker_confidence'] for s in group) / len(group),
            'merged_from': len(group),
            'silence_after': last.get('silence_after', 0.0)
        }
    
    def _detect_gender_for_segments(self, segments: List[Dict]) -> List[Dict]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ–ª –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –≥–æ–ª–æ—Å–∞
        
        Args:
            segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∞—É–¥–∏–æ
            
        Returns:
            segments: —Å–µ–≥–º–µ–Ω—Ç—ã —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–æ–ª–µ –∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–º –≥–æ–ª–æ—Å–æ–º
        """
        self.logger.info("üé≠ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–ª–∞ —Å–ø–∏–∫–µ—Ä–æ–≤...")
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤
        self.used_voices = {'male': 0, 'female': 0}
        speaker_genders = {}  # –ö—ç—à –¥–ª—è —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å–ø–∏–∫–µ—Ä–æ–≤
        
        for segment in segments:
            speaker_id = segment['speaker']
            
            # –ï—Å–ª–∏ —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ –ø–æ–ª –¥–ª—è —ç—Ç–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à
            if speaker_id in speaker_genders:
                gender = speaker_genders[speaker_id]
            else:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–ª –ø–æ –∞—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç—É
                gender = self._analyze_voice_gender(segment['path'])
                speaker_genders[speaker_id] = gender
            
            # –ù–∞–∑–Ω–∞—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –≥–æ–ª–æ—Å –¥–ª—è —ç—Ç–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
            voice_id = self._assign_voice_for_speaker(speaker_id, gender)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Å–µ–≥–º–µ–Ω—Ç
            segment['gender'] = gender
            segment['voice_id'] = voice_id
            
            self.logger.debug(f"üé≠ {speaker_id}: {gender}, –≥–æ–ª–æ—Å: {voice_id}")
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        gender_stats = {}
        for segment in segments:
            gender = segment['gender']
            gender_stats[gender] = gender_stats.get(gender, 0) + 1
        
        self.logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª–æ–≤: {gender_stats}")
        
        return segments
    
    def _analyze_voice_gender(self, audio_path: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–ª –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –ø–æ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
        
        Args:
            audio_path: –ø—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
            
        Returns:
            str: 'male' –∏–ª–∏ 'female'
        """
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
            y, sr = librosa.load(audio_path, sr=None)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —á–∞—Å—Ç–æ—Ç—É (F0) - –∫–ª—é—á–µ–≤–æ–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –ø–æ–ª–∞
            pitches, magnitudes = librosa.piptrack(y=y, sr=sr, threshold=0.1)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è F0
            f0_values = []
            for t in range(pitches.shape[1]):
                index = magnitudes[:, t].argmax()
                pitch = pitches[index, t]
                if pitch > 0:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –Ω—É–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    f0_values.append(pitch)
            
            if not f0_values:
                # Fallback: –∞–Ω–∞–ª–∏–∑ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
                return self._analyze_spectral_features(y, sr)
            
            # –ú–µ–¥–∏–∞–Ω–Ω–∞—è –æ—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞
            median_f0 = np.median(f0_values)
            
            self.logger.debug(f"üéµ F0 –º–µ–¥–∏–∞–Ω–∞: {median_f0:.1f} Hz")
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –æ—Å–Ω–æ–≤–Ω–æ–π —á–∞—Å—Ç–æ—Ç–µ
            # –ú—É–∂—á–∏–Ω—ã: –æ–±—ã—á–Ω–æ 85-180 Hz
            # –ñ–µ–Ω—â–∏–Ω—ã: –æ–±—ã—á–Ω–æ 165-265 Hz
            if median_f0 < 150:
                return 'male'
            elif median_f0 > 200:
                return 'female'
            else:
                # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–∞—è –∑–æ–Ω–∞ - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                return self._analyze_spectral_features(y, sr)
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–ª–∞: {e}")
            # Fallback: —Å–ª—É—á–∞–π–Ω–æ–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Å—Ç–æ–π —ç–≤—Ä–∏—Å—Ç–∏–∫–∏
            return 'male' if len(audio_path) % 2 == 0 else 'female'
    
    def _analyze_spectral_features(self, y: np.ndarray, sr: int) -> str:
        """
        –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–ª–∞
        
        Args:
            y: –∞—É–¥–∏–æ —Å–∏–≥–Ω–∞–ª
            sr: —á–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏
            
        Returns:
            str: 'male' –∏–ª–∏ 'female'
        """
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥ (—è—Ä–∫–æ—Å—Ç—å –∑–≤—É–∫–∞)
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
            mean_centroid = np.mean(spectral_centroids)
            
            # –í—ã—á–∏—Å–ª—è–µ–º MFCC (–º–µ–ª-—á–∞—Å—Ç–æ—Ç–Ω—ã–µ –∫–µ–ø—Å—Ç—Ä–∞–ª—å–Ω—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã)
            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
            mean_mfcc = np.mean(mfccs, axis=1)
            
            self.logger.debug(f"üéµ –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥: {mean_centroid:.1f} Hz")
            
            # –ñ–µ–Ω—Å–∫–∏–µ –≥–æ–ª–æ—Å–∞ –æ–±—ã—á–Ω–æ –∏–º–µ—é—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥
            # –∏ –¥—Ä—É–≥–∏–µ MFCC —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            if mean_centroid > 2500:  # –í—ã—Å–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥
                return 'female'
            elif mean_centroid < 1500:  # –ù–∏–∑–∫–∏–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥
                return 'male'
            else:
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º MFCC –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è
                # –í—Ç–æ—Ä–æ–π MFCC –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —á–∞—Å—Ç–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å –ø–æ–ª–æ–º
                if len(mean_mfcc) > 1 and mean_mfcc[1] > 0:
                    return 'female'
                else:
                    return 'male'
                    
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return 'male'  # Fallback –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def _assign_voice_for_speaker(self, speaker_id: str, gender: str) -> str:
        """
        –ù–∞–∑–Ω–∞—á–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –≥–æ–ª–æ—Å –¥–ª—è —Å–ø–∏–∫–µ—Ä–∞
        
        Args:
            speaker_id: –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–ø–∏–∫–µ—Ä–∞
            gender: –ø–æ–ª —Å–ø–∏–∫–µ—Ä–∞ ('male' –∏–ª–∏ 'female')
            
        Returns:
            str: –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–æ–≥–æ –≥–æ–ª–æ—Å–∞
        """
        if gender not in self.voice_mapping:
            gender = 'male'  # Fallback
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –≥–æ–ª–æ—Å –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ–ª–∞
        available_voices = self.voice_mapping[gender]
        voice_index = self.used_voices[gender] % len(available_voices)
        voice_id = available_voices[voice_index]
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ø–∏–∫–µ—Ä–∞ —Ç–æ–≥–æ –∂–µ –ø–æ–ª–∞
        self.used_voices[gender] += 1
        
        return voice_id
    
    def _extract_voice_samples_for_cloning(self, audio_path: str, segments: List[Dict]) -> None:
        """
        Extract voice samples for each speaker for voice cloning
        
        Args:
            audio_path: path to the original audio file
            segments: list of speaker segments
        """
        try:
            self.logger.info("üé§ Extracting voice samples for voice cloning...")
            
            # Group segments by speaker
            speakers_segments = {}
            for segment in segments:
                speaker_id = segment['speaker']
                if speaker_id not in speakers_segments:
                    speakers_segments[speaker_id] = []
                speakers_segments[speaker_id].append(segment)
            
            # Extract samples for each speaker
            for speaker_id, speaker_segments in speakers_segments.items():
                if speaker_id in self.voice_samples_cache:
                    continue  # Already have sample for this speaker
                
                # Find the longest segment for this speaker (best quality sample)
                best_segment = max(speaker_segments, key=lambda x: x['duration'])
                
                # Only extract if segment is long enough (minimum 3 seconds for good voice cloning)
                if best_segment['duration'] >= 3.0:
                    sample_path = best_segment['path']
                    
                    # Extract voice characteristics using voice cloner
                    characteristics = self.voice_cloner.extract_voice_characteristics(
                        sample_path, speaker_id=speaker_id
                    )
                    
                    if characteristics:
                        # Cache the voice sample and characteristics
                        self.voice_samples_cache[speaker_id] = {
                            'sample_path': sample_path,
                            'characteristics': characteristics,
                            'duration': best_segment['duration'],
                            'confidence': best_segment.get('speaker_confidence', 0.5)
                        }
                        
                        self.logger.info(f"‚úÖ Voice sample extracted for {speaker_id}: {best_segment['duration']:.1f}s")
                    else:
                        self.logger.warning(f"‚ö†Ô∏è Failed to extract characteristics for {speaker_id}")
                else:
                    self.logger.warning(f"‚ö†Ô∏è Segment too short for {speaker_id}: {best_segment['duration']:.1f}s")
            
            # Add voice sample information to segments
            for segment in segments:
                speaker_id = segment['speaker']
                if speaker_id in self.voice_samples_cache:
                    segment['voice_sample_path'] = self.voice_samples_cache[speaker_id]['sample_path']
                    segment['voice_characteristics'] = self.voice_samples_cache[speaker_id]['characteristics']
            
            self.logger.info(f"üé≠ Voice samples extracted for {len(self.voice_samples_cache)} speakers")
            
        except Exception as e:
            self.logger.error(f"‚ùå Error extracting voice samples: {e}")
    
    def get_voice_sample(self, speaker_id: str) -> Optional[Dict]:
        """
        Get cached voice sample for a speaker
        
        Args:
            speaker_id: speaker identifier
            
        Returns:
            dict: voice sample information or None if not found
        """
        return self.voice_samples_cache.get(speaker_id)
    
    def get_all_voice_samples(self) -> Dict[str, Dict]:
        """
        Get all cached voice samples
        
        Returns:
            dict: mapping of speaker_id to voice sample information
        """
        return self.voice_samples_cache.copy()